{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde6229d",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "* XGBoost (Extreme Gradient Boosting) is an optimized and efficient gradient boosting framework. It follows the usual Gradient Boosting process, but includes several key concepts to improve model performance, speed, and scalability.\n",
    "    * Regularization: XGBoost includes L1 and L2 regularization terms to the algorithm's objective function to prevent overfitting and control tree complexity\n",
    "    * Loss Functions: XGBoost gives users the option to define loss functions based on the specific problem, allowing flexibility for custom tasks\n",
    "    * Tree Construction: XGBoost uses a histogram-based approach to find the best splits for the dataset. This involves precomputing statistics on features and storing them in histograms to speed up the process. XGBoost also handles sparse data by using a compressed format to skip missing/empty values\n",
    "    * Missing Values: XGBoost handles missing values during the training step and can learn how to handle them based on the given data\n",
    "    * Scalability: Parallel and distributed computing is supported, making it efficient for larger datasets\n",
    "    * Categorical Features: Categorical features are encoded as integers and splits are done on these encodings (no need to do One-Hot Encoding)\n",
    "\n",
    "XGBoost is currently regarded as one of the most powerful and effective machine learning algorithms. There are other packages that use the Gradient Boosting framework and are worth checking out (e.g. LightGBM, CatBoost). LightGBM focuses on optimizing memory usage and CatBoost (Categorical Boosting) handles categorical features more effectively by using a permutation-based approach to reduce overfitting from categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd5ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386dea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2fbca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c32236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Training R-Squared: 0.9280429184163838\n",
      "Testing R-Squared: 0.8436781914859918\n",
      "Mean Squared Error: 0.20484550137161106\n"
     ]
    }
   ],
   "source": [
    "# Create an XGBoost regressor\n",
    "model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train a model using the best parameters\n",
    "best_model = xgb.XGBRegressor(random_state=42, **best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Training R-Squared: {best_model.score(X_train, y_train)}\")\n",
    "print(f\"Testing R-Squared: {best_model.score(X_test, y_test)}\")\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f4681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
